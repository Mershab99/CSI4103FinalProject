\documentclass{article}
\usepackage{amsmath} % For math symbols
\usepackage{array} % For better table formatting
\usepackage{booktabs} % For professional-looking tables

\begin{document}

\section*{Covariance Matrix Example}

The covariance between two variables \(X\) and \(Y\) is calculated as:
\[
\text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})
\]

The covariance matrix for a set of variables \(X_1, X_2, \dots, X_k\) is a symmetric matrix where each entry \(\text{Cov}(X_i, X_j)\) represents the covariance between the variables \(X_i\) and \(X_j\).

\[
\mathbf{Cov} = 
\begin{bmatrix}
\text{Var}(X_1) & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_k) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2) & \cdots & \text{Cov}(X_2, X_k) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_k, X_1) & \text{Cov}(X_k, X_2) & \cdots & \text{Var}(X_k) \\
\end{bmatrix}
\]

\section*{Example Calculation}

Consider the following dataset with three features: Closing Price (\(X_1\)), Volume (\(X_2\)), and Market Cap (\(X_3\)):

\[
\begin{array}{ccc}
\toprule
\textbf{Day} & \textbf{Closing Price (\(X_1\))} & \textbf{Volume (\(X_2\))} & \textbf{Market Cap (\(X_3\))} \\
\midrule
1 & 100 & 2000 & 5000 \\
2 & 110 & 2200 & 5500 \\
3 & 105 & 2100 & 5250 \\
4 & 115 & 2300 & 5750 \\
\bottomrule
\end{array}
\]

The mean for each feature is calculated as:
\[
\bar{X}_1 = \frac{100 + 110 + 105 + 115}{4} = 107.5, \quad
\bar{X}_2 = \frac{2000 + 2200 + 2100 + 2300}{4} = 2150, \quad
\bar{X}_3 = \frac{5000 + 5500 + 5250 + 5750}{4} = 5375
\]

Subtract the mean from each value to create the mean-centered matrix:

\[
\begin{array}{ccc}
\toprule
\textbf{Day} & X_1 - \bar{X}_1 & X_2 - \bar{X}_2 & X_3 - \bar{X}_3 \\
\midrule
1 & -7.5 & -150 & -375 \\
2 & 2.5 & 50 & 125 \\
3 & -2.5 & -50 & -125 \\
4 & 7.5 & 150 & 375 \\
\bottomrule
\end{array}
\]

The covariance matrix is then calculated by computing the covariance for each pair of features. For example, the covariance between \(X_1\) and \(X_2\) is:
\[
\text{Cov}(X_1, X_2) = \frac{1}{4-1} \sum_{i=1}^{4} (X_{1,i} - \bar{X}_1)(X_{2,i} - \bar{X}_2)
\]
\[
\text{Cov}(X_1, X_2) = \frac{1}{3} \left[ (-7.5)(-150) + (2.5)(50) + (-2.5)(-50) + (7.5)(150) \right] = 3750
\]

The full covariance matrix is:
\[
\mathbf{Cov} = 
\begin{bmatrix}
31.25 & 3750 & 937.5 \\
3750 & 250000 & 62500 \\
937.5 & 62500 & 15625 \\
\end{bmatrix}
\]

\section*{Interpretation}

- The diagonal entries represent the variances of each feature.
- The off-diagonal entries represent the covariances between pairs of features.
- A high positive covariance indicates a strong positive relationship, while a high negative covariance indicates a strong negative relationship.



\section*{Eigenvalues and Eigenvectors}

The eigenvalues and eigenvectors of the covariance matrix are fundamental in understanding the principal components of the dataset. These are calculated by solving the eigenvalue equation:

\[
\mathbf{Cov} \mathbf{v} = \lambda \mathbf{v}
\]

Where:
\begin{itemize}
    \item \(\mathbf{Cov}\) is the covariance matrix.
    \item \(\lambda\) is an eigenvalue.
    \item \(\mathbf{v}\) is the corresponding eigenvector.
\end{itemize}

The eigenvalues represent the amount of variance explained by each principal component, and the eigenvectors represent the directions of the principal components.

\subsection*{Example Calculation}

The covariance matrix from the previous example is:

\[
\mathbf{Cov} = 
\begin{bmatrix}
31.25 & 3750 & 937.5 \\
3750 & 250000 & 62500 \\
937.5 & 62500 & 15625 \\
\end{bmatrix}
\]

To find the eigenvalues, solve the characteristic equation:
\[
\text{det}(\mathbf{Cov} - \lambda \mathbf{I}) = 0
\]

Expanding this determinant leads to a cubic equation in \(\lambda\), which can be solved numerically or symbolically. The eigenvalues for this example are:

\[
\lambda_1 = 264281.25, \quad \lambda_2 = 21875, \quad \lambda_3 = 20
\]

Next, for each eigenvalue \(\lambda_i\), solve the equation \((\mathbf{Cov} - \lambda_i \mathbf{I}) \mathbf{v} = 0\) to find the corresponding eigenvector \(\mathbf{v}_i\).

The eigenvectors are:

\[
\mathbf{v}_1 = 
\begin{bmatrix}
0.014 \\
0.707 \\
0.707
\end{bmatrix}, \quad
\mathbf{v}_2 = 
\begin{bmatrix}
-0.707 \\
0.5 \\
0.5
\end{bmatrix}, \quad
\mathbf{v}_3 = 
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
\]

\subsection*{Interpretation}

- \(\lambda_1 = 264281.25\) is the largest eigenvalue, indicating that the first principal component explains the most variance.
- The corresponding eigenvector \(\mathbf{v}_1 = [0.014, 0.707, 0.707]^T\) points in the direction of the first principal component.
- Smaller eigenvalues (\(\lambda_2, \lambda_3\)) represent directions with less variance.

By normalizing the eigenvectors, we obtain an orthonormal basis for the data's principal components.




\end{document}
