\documentclass{article}
\usepackage{amsmath} % For math symbols
\usepackage{array} % For better table formatting
\usepackage{booktabs} % For professional-looking tables

\begin{document}


\section*{5.0 Algorithm Explanation}

This section provides an overview of the process for constructing the covariance matrix, computing eigenvalues and eigenvectors, selecting principal components, and interpreting the results. These steps collectively form the foundation for Principal Component Analysis (PCA) in this context.

\section*{5.2 Covariance Matrix Construction}

The covariance matrix is calculated as follows. Given a dataset with features \(X_1, X_2, \dots, X_k\) and \(n\) observations, the covariance between two variables \(X_i\) and \(X_j\) is defined as:

\[
\text{Cov}(X_i, X_j) = \frac{1}{n-1} \sum_{k=1}^n (X_{i,k} - \bar{X}_i)(X_{j,k} - \bar{X}_j)
\]

The covariance matrix for all features is then expressed as:

\[
\mathbf{Cov} = 
\begin{bmatrix}
\text{Var}(X_1) & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_k) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2) & \cdots & \text{Cov}(X_2, X_k) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_k, X_1) & \text{Cov}(X_k, X_2) & \cdots & \text{Var}(X_k) \\
\end{bmatrix}
\]

\subsection*{Example}

Consider the following dataset:

\[
\begin{array}{ccc}
\toprule
\textbf{Day} & \textbf{Feature \(X_1\)} & \textbf{Feature \(X_2\)} & \textbf{Feature \(X_3\)} \\
\midrule
1 & 100 & 2000 & 5000 \\
2 & 110 & 2200 & 5500 \\
3 & 105 & 2100 & 5250 \\
4 & 115 & 2300 & 5750 \\
\bottomrule
\end{array}
\]

After mean-centering the data and applying the covariance formula, the covariance matrix becomes:

\[
\mathbf{Cov} = 
\begin{bmatrix}
31.25 & 3750 & 937.5 \\
3750 & 250000 & 62500 \\
937.5 & 62500 & 15625 \\
\end{bmatrix}
\]

\section*{5.3 Eigenvalue and Eigenvector Computation}

The eigenvalues and eigenvectors of the covariance matrix are computed by solving the equation:

\[
\mathbf{Cov} \mathbf{v} = \lambda \mathbf{v}
\]

Where:
\begin{itemize}
    \item \(\mathbf{Cov}\) is the covariance matrix.
    \item \(\lambda\) is an eigenvalue.
    \item \(\mathbf{v}\) is the corresponding eigenvector.
\end{itemize}

\subsection*{Example}

For the covariance matrix:

\[
\mathbf{Cov} = 
\begin{bmatrix}
31.25 & 3750 & 937.5 \\
3750 & 250000 & 62500 \\
937.5 & 62500 & 15625 \\
\end{bmatrix}
\]

The eigenvalues are:

\[
\lambda_1 = 264281.25, \quad \lambda_2 = 21875, \quad \lambda_3 = 20
\]

The eigenvectors are:

\[
\mathbf{v}_1 = 
\begin{bmatrix}
0.014 \\
0.707 \\
0.707
\end{bmatrix}, \quad
\mathbf{v}_2 = 
\begin{bmatrix}
-0.707 \\
0.5 \\
0.5
\end{bmatrix}, \quad
\mathbf{v}_3 = 
\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
\]

\section*{5.4 Principal Component Selection}

Principal components are selected based on their corresponding eigenvalues. Eigenvalues represent the amount of variance explained by each component. Components with higher eigenvalues capture more variance.

\subsection*{Selection Criterion}

The selection criterion is to retain the components that explain a significant proportion of the total variance. This is determined as:

\[
\text{Explained Variance Ratio} = \frac{\lambda_i}{\sum_{j=1}^k \lambda_j}
\]

For the example, the explained variance ratios are:

\[
\frac{\lambda_1}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{264281.25}{286176.25} \approx 0.923
\]
\[
\frac{\lambda_2}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{21875}{286176.25} \approx 0.076
\]
\[
\frac{\lambda_3}{\lambda_1 + \lambda_2 + \lambda_3} = \frac{20}{286176.25} \approx 0.00007
\]

Thus, the first principal component captures 92.3\% of the variance and is selected as the most significant component.

\section*{5.5 Interpretation of Components}

The selected principal components represent the directions of maximum variance in the data. These can be interpreted to deduce underlying market factors.

\subsection*{Example Interpretation}

For the first principal component (\(\mathbf{v}_1\)):

\[
\mathbf{v}_1 = 
\begin{bmatrix}
0.014 \\
0.707 \\
0.707
\end{bmatrix}
\]

This indicates that the first principal component is heavily influenced by the features \(X_2\) (Volume) and \(X_3\) (Market Cap), with a smaller contribution from \(X_1\) (Closing Price).

The large variance captured by this component suggests that market activity (Volume and Market Cap) dominates the variance structure of the dataset.

\end{document}

